---
version: "3.9"

services:

  ####################################################################################################
  # MySQL : Hadoop Metastore, Airflow Metastore
  ####################################################################################################
  mysql-server:
    image: mysql:8.0.36-bullseye
    container_name: mysql-server
    hostname: mysql-server
    env_file:
      - ./.envs/.local/.mysql
    environment:
      TZ: Asia/Seoul
    ports:
      - "3306:3306"
    volumes:
      - hive_metastore_data:/data
      - ./init-sql/mysql/docker-entrypoint-initdb.d:/docker-entrypoint-initdb.d
    restart: unless-stopped  
    healthcheck:
      # mysqladmin operations docs: https://dev.mysql.com/doc/refman/8.0/en/mysqladmin.html
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-proot"]
      timeout: 10s
      retries: 10
    networks:
      hadoop-cluster-net:
        ipv4_address: 172.28.1.100
    # command:
      # mysql 7.X 버전까지는 세 command 모두 실행.  
      # mysql 8.X 버전 변경 사항: https://www.lesstif.com/dbms/mysql-8-character-set-collation-91947077.html
      # - --character-set-server=utf8
      # - --collation-server=utf8_general_ci
      # - --skip-character-set-client-handshake           
    extra_hosts:
      - "mysql-server:172.28.1.100"
      - "master-server:172.28.1.101"
      - "slave-server-1:172.28.1.102"
      - "slave-server-2:172.28.1.103"

  ####################################################################################################
  # Airflow
  ####################################################################################################
  # redis:
  #     image: redis:5.0.5
  #     command: redis-server --requirepass redispass

  # flower:
  #     build: ./docker/airflow
  #     restart: always
  #     depends_on:
  #         - redis
  #     environment:
  #         - EXECUTOR=Celery
  #         - REDIS_PASSWORD=redispass
  #     ports:
  #         - "5555:5555"
  #     command: flower

  x-hadoop-common:
    &hadoop-common
    build:
      context: .
      dockerfile: ./docker/base-hadoop/Dockerfile
    image: keyhong/base-hadoop:1.0
    extra_hosts:
      # 172.28.1.0: Network IP, 172.28.1.255: Broadcast IP
      # CIDR가 172.28.0.0/16이기 때문에 172.28.*.1 ~ 172.28.*.254 까지 사용 가능합니다(*는 일치)  
      - "mysql-server:172.28.1.100"
      - "master-server:172.28.1.101"
      - "slave-server-1:172.28.1.102"
      - "slave-server-2:172.28.1.103"

  master-server:
    <<: *hadoop-common
    build:
      context: .
      dockerfile: ./docker/master-server/Dockerfile
    image: keyhong/master-server     
    container_name: master-server
    hostname: master-server
    depends_on:
      - x-hadoop-common
      - mysql-server
    expose:
      - "2888" # zookeepr quorum
      - "3888" # zookeepr election
      - "8020" # dfs.namenode.rpc-address.docker-cluster.nn1
      - "8033" # yarn.resourcemanager.address
      - "8042" # yarn.nodemanager.address
      - "8485" # dfs.journalnode.rpc-address
      - "8089" # yarn.web-proxy.address
    ports:
      # - "2181:2181"
      # - "4040:4040" # spark 
      # - "8080:8080" # airflow web
      - "8088:8088" # yarn.resourcemanager.webapp.address.rm1
      - "9870:9870"
      - "10000:10000"
      - "18080:18080" # spark-history
      - "19888:19888" # job-history
      - "5071:50070" # dfs.namenode.http-address.docker-cluster.nn1
    volumes:
      - namenode_data:/data
    # healthcheck: 
    #   test: ["CMD", "curl", "-f", "http://localhost:9870/"]
    #   timeout: 10s
    #   retries: 10
    networks:
      hadoop-cluster-net:
        ipv4_address: 172.28.1.101

  slave-server-1:
    <<: *hadoop-common
    build:
      context: .
      dockerfile: ./docker/slave-server-1/Dockerfile
    image: keyhong/slave-server-1:1.0
    container_name: slave-server-1
    hostname: slave-server-1
    depends_on:
      - master-server
    expose:
      - "2888" # zookeepr quorum
      - "3888" # zookeepr election
      - "8020" # dfs.namenode.rpc-address.docker-cluster.nn2(HA)
      - "8033" # yarn.resourcemanager.address(HA)
      - "8042" # yarn.nodemanager.address
      - "8485" # dfs.journalnode.rpc-address(HA)
    ports:
      # - "2182:2181"
      - "8089:8088" # yarn.resourcemanager.webapp.address.rm2(HA)
      - "8081:8081"
      - "9864:9864"
      - "5070:50070" # dfs.namenode.http-address.docker-cluster.nn2(HA)
    volumes:
      - datanode_data_1:/data/
    networks:
      hadoop-cluster-net:
        ipv4_address: 172.28.1.102

  slave-server-2:
    <<: *hadoop-common
    build:
      context: .
      dockerfile: ./docker/slave-server-2/Dockerfile
    image: keyhong/slave-server-2:1.0
    container_name: slave-server-2
    hostname: slave-server-2
    depends_on:
      - master-server
    expose:
      - "2888" # zookeepr quorum
      - "3888" # zookeepr election
      - "8042" # yarn.nodemanager.address
      - "8485" # dfs.journalnode.rpc-address      
    ports:
      # - "2183:2181"
      - "8043:8042"
      - "8082:8081"
      - "9865:9864"
    volumes:
      - datanode_data_2:/data/
    networks:
      hadoop-cluster-net:
        ipv4_address: 172.28.1.103


  # spark:
  #   image: hadoop-hive-spark-dev
  #   hostname: dev
  #   environment:
  #     SPARK_master_HOST: 172.28.1.2
  #     SPARK_LOCAL_IP: 172.28.1.7
  #     SPARK_LOCAL_HOSTNAME: dev
  #   volumes:
  #     - ./dev/home:/home/jupyter
  #   networks:
  #     hadoop-cluster-net:
  #       ipv4_address: 172.28.1.7
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - capabilities: [gpu]

volumes:
  namenode_data:
  namesecondary:
  datanode_data_1:
  datanode_data_2:
  hive_metastore_data:
  airflow_data: 

networks:
  hadoop-cluster-net:
    ipam:
      driver: default
      config:
        - subnet: 172.28.1.0/16
...